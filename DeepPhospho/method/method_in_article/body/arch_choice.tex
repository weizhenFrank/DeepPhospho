To illustrate our model design's efficacy, we compare our model architecture with removed LSTM module and removed transformer module. In further, we compare our model with the replacement of LSTM module with convolutional neural network (CNN) module.The CNN module is built like the ResNet34~\cite{he2015deep} except the kernel size of the first convolution layer changed to be 9, and the kernel size in residual block changed to be 7, and it is composed of 3 residual blocks.
So that there are four models in the ablation study, that is DeepPhospho, LSTM, Transformer and CNN$+$Transformer. 

We do the experiments both on ion intensity dataset and RT dataset. For ion intensity dataset, we use Jeff and R2P2\_DDA~\cite{leutert2019r2} dataset. For RT, we use R2P2 yeast~\cite{leutert2019r2} dataset. We split the dataset into training : validation : test = 8 : 1 : 1, and we tune the hyper-parameter and select the best model in the validation set, reporting the results on the test set.

From the results we could see that, in the test set, our DeepPhospho model achieve median pcc is $0.949$ in Jeff, compared to $0.934$, $0.901$ and $0.937$ of LSTM, Transformer and CNN$+$Transformer. Other datasets also prove that our DeepPhospho's architecture is the best.

Additionally, as Transformer module could capture the long term dependendy better by attention than LSTM~\cite{vaswani2017attention}, we expect the our model has better performance in the longer sequence compared the model based on the LSTM architecture. We select the peptide whose length is more than 40 and could see that the gap between our model and LSTM increase. In the longer sequence, the median pcc of our model is $0.926$, but LSTM's is only $0.894$.
The full comparison results are in supplementary materials. From the result, we could conclude that our model composed of LSTM module and Transformer module is better than any single component, and if we replace the LSTM module with CNN module, we could not obtain the better result.

