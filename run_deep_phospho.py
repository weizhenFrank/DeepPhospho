import argparse
import copy
import datetime
import os
from os.path import join as join_path

from deep_phospho.proteomics_utils import rapid_kit as rk
from deep_phospho.train_pred_utils.runner import DeepPhosphoRunner

HelpMSG = '''
This script integrates all functions of DeepPhospho in one.
To run this, at least two files are needed:
    1. a file contains both spectra and (i)RT, and the following files are valid:
        I. spectral library from Spectronaut (either directDIA or DDA is fine)
        II. msms.txt file generated by MaxQuant
        III. tsv file from EasyPQP
        [for more information about training data, please see -tf (--train_file) and -tt (--train_file_type)]
    2. a file contains under-predicted peptide precursors, and the following files are valid:
        I. spectral library from Spectronaut (either directDIA or DDA)
        II. search result from Spectronaut
        III. msms.txt or evidence.txt file generated by MaxQuant
        IV. tsv file from EasyPQP
        V. two-column tab-separated file with column name "sequence" and "charge" (Some common peptide formats are supported)
        [for more information about prediction input data, please see -pf (--pred_file) and -pt (--pred_file_type)]
We also provided some suggestions about the preperation of these two files, please visit our GitHub repository for more information.

The detailed help message about each argument is listed below.
If you have any question or any suggestion, please contact us.

At last, thank you for using DeepPhospho
[DeepPhospho repository] https://github.com/weizhenFrank/DeepPhospho

'''


def init_arg_parser():
    parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter, description=HelpMSG)

    # work folder
    parser.add_argument('-w', '--work_dir', metavar='directory', type=str, default=None,
                        help='Task will start in this directory')
    # task name
    parser.add_argument('-t', '--task_name', metavar='str', type=str, default=None,
                        help='Task name will be used to name some folders')

    # train file
    parser.add_argument('-tf', '--train_file', metavar='path', type=str, default=None,
                        help='''Train file can be either a spectral library from Spectronaut or msms.txt from MaxQuant, or tsv file from EasyPQP''')
    # train file type
    parser.add_argument('-tt', '--train_file_type', metavar='str', default=None,
                        help='''To use Spectronaut library, set this to "SNLib"
Use MaxQuant msms.txt file, set this to "MQ1.5" for MaxQuant version <= 1.5, and "MQ1.6" for version >= 1.6
Use tsv file from EasyPQP, set this to "EasyPQP"''')

    # train file split
    parser.add_argument('-tr', '--train_split_ratio', metavar='str', default="8,2",
                        help='''The ratio of split training data, for training, validation, and test.
Set to "number,number" to split file into two parts for training and validation.
Set to "number,number,number" to split file into three parts for training, validation, and test.
The numbers can be any positive value at any scale, and will be normalized when processing.
Default is 8,2''')

    # pred file
    parser.add_argument('-pf', '--pred_file', metavar='path', type=str, nargs='*', action='append',  # required=True,
                        help='''File contains under-predicted peptide precursors may has multi-source:
I. spectral library from Spectronaut
II. search result from Spectronaut
III. msms.txt or evidence.txt from MaxQuant
IV. tsv file from EasyPQP
V. any tab-separated file with two columns "sequence" and "charge"
Either -pf file_1 file_2 ... or -pf file_1 -pf file_2 ... or mix above two is fine for this argument''')
    # pred file type
    parser.add_argument('-pt', '--pred_file_type', metavar='str', type=str, nargs='*', action='append',  # required=True,
                        help='''The source of prediction input file or peptide format
I. "SNLib" for Spectronaut library
II. "SNResult" for Spectronaut result
III. "MQ1.5" or "MQ1.6" for msms.txt/evidence.txt from MaxQuant version <= 1.5 or >= 1.6
IV. "EasyPQP" for tsv file generated by EasyPQP
V. for peptide list file, the modified peptides in the following format are valid
    a. "PepSN13" is Spectronaut 13+ peptide format like _[Acetyl (Protein N-term)]M[Oxidation (M)]LSLS[Phospho (STY)]PLK_
    b. "PepMQ1.5" is MaxQuant 1.5- peptide format like _(ac)GS(ph)QDM(ox)GS(ph)PLRET(ph)RK_
    c. "PepMQ1.6" is MaxQuant 1.6+ peptide format like _(Acetyl (Protein N-term))TM(Oxidation (M))DKS(Phospho (STY))ELVQK_
    d. "PepUniMod" is UniMod peptide format like (UniMod:1)AHAC(UniMod:4)WPS(UniMod:21)PYM(UniMod:35)K
    e. "PepComet" is Comet peptide format like n#DFM*SPKFS@LT@DVEY@PAWCQDDEVPITM*QEIR
    f. "PepDP" is DeepPhospho used peptide format like *1ED2MCLK
When -pt is passed once and multi -pf are used, the single format will be assigned to all input files. 
If the input files have different formats, the same number of -pt is needed''')

    # device
    parser.add_argument('-d', '--device', metavar='cpu|0|1|...', type=str, default='cpu',
                        help='Use which device. This can be [cpu] or any integer (0, 1, 2, ...) to use corresponded GPU')
    # epoch
    parser.add_argument('-e', '--epoch', metavar='int', type=int, default=30,
                        help='Train how much epochs for both ion an RT models. '
                             'This will only be effective for one of ion or RT model when -ie (--ion_epoch) or -re (--rt_epoch) is provided. '
                             'Or no effect when both -ie and -re are provided. '
                             'Default is 30')
    parser.add_argument('-ie', '--ion_epoch', metavar='int', type=int, default=None,
                        help='Train how much epochs on ion model. Default is 30')
    parser.add_argument('-re', '--rt_epoch', metavar='int', type=int, default=None,
                        help='Train how much epochs on RT model. Default is 30')
    # batch size
    parser.add_argument('-ibs', '--ion_batch_size', metavar='int', type=int, default=64,
                        help='Batch size for ion model training. Default is 64')
    parser.add_argument('-rbs', '--rt_batch_size', metavar='int', type=int, default=128,
                        help='Batch size for RT model training. Default is 128')
    # initial learning rate
    parser.add_argument('-lr', '--learning_rate', metavar='float', type=float, default=0.0001,
                        help='Initial learning rate for two models. '
                             'Default is 0.0001 while a smaller value is recommanded if the size of training data is small (e.g. hundreds of precursors)')
    # pep length
    parser.add_argument('-ml', '--max_len', metavar='int', type=int, default=74,
                        help='The max peptide length for input dataset. '
                             'Default is 74 to avoid error but 54 (or 25) is recommended as default setting of Spectronaut (or Skyline)')
    # rt scale
    parser.add_argument('-rs', '--rt_scale', metavar='low,high', type=str, default='*-100,200',
                        help='Define the lower and upper limitations for RT model. '
                             'Separate two numbers with a comma like `0,10`. '
                             'And a `*` will be needed for negative number, like `*-100,200`. '
                             'Default is `*-100,200` (-100 to 200)')
    # rt ensemble
    parser.add_argument('-en', '--ensemble_rt', default=False, action='store_true',
                        help='Use ensemble to improve RT prediction or not')

    # train task
    parser.add_argument('-train', '--train', default=1, type=int,
                        help='''Train a new model (1) or not (0). Default 1''')
    # pred task
    parser.add_argument('-pred', '--predict', default=1, type=int,
                        help='''Perform prediction (1) or not (0). Default 1''')

    # pre-trained models
    _pretrain_ion = os.path.join('.', 'PretrainParams', 'IonModel', 'best_model.pth')
    _pretrain_ion = _pretrain_ion if os.path.exists(_pretrain_ion) else ''
    parser.add_argument('-pretrain_ion', '--pretrain_ion_model', metavar='path', type=str, default=_pretrain_ion,
                        help='Fine-tune on pre-trained ion model parameters or directly use this model to do prediction. '
                             'This will be automatically filled-in if pre-trained models param file is existed as "PretrainParams/IonModel/best_model.pth". '
                             'If you don\'t want to use pre-trained model param anywhere, please explicitly define this argument and set value to `/`')
    for l in [4, 5, 6, 7, 8]:
        _pretrain_rt = os.path.join('.', 'PretrainParams', 'RTModel', f'{l}.pth')
        _pretrain_rt = _pretrain_rt if os.path.exists(_pretrain_rt) else ''
        parser.add_argument(f'-pretrain_rt_{l}', f'--pretrain_rt_model_{l}', metavar='path', type=str, default=_pretrain_rt,
                            help=f'Fine-tune on pre-trained RT model parameters (with {l} encoder layer) or directly use pre-trained models to do prediction. '
                                 f'This will be automatically filled-in if pre-trained models param files are existed as "PretrainParams/IonModel/(layer_number).pth". '
                                 f'If -en (-ensemble_rt) is not used, only -rt_model_8 is required'
                                 f'If you don\'t want to use pre-trained model param anywhere, please explicitly define this argument and set value to `/`')

    # skip fine-tuning
    parser.add_argument('-skip_ion_finetune', '--skip_ion_finetune', default=False, action='store_true',
                        help='Partial training option. '
                             'When this argument is passed, ion model fine-tuning step will be skipped. '
                             'While RT model training will still be performed if -skip_rt_finetune_{layer_number} is not passed. '
                             'This will be useful if you already have a fine-tuned ion model but have no or only some of fine-tuned RT models, '
                             'and still want to use DeepPhospho runner but not individual train/prediction scripts. ')
    for l in [4, 5, 6, 7, 8]:
        parser.add_argument(f'-skip_rt_finetune_{l}', f'--skip_rt_finetune_{l}', default=False, action='store_true',
                            help='Partial training option. '
                                 f'When this argument is passed, RT model fine-tuning step for layer {l} will be skipped. '
                                 f'Use existed RT model parameters (with {l} encoder layer) instead of training a new one. '
                                 'This will be useful if you already have some fine-tuned RT model but not enough for ensemble, '
                                 'or RT model has already been trained but ion model is need to be fine-tuned. '
                                 'In this case, you can still use DeepPhospho runner but not individual train/prediction scripts. ')
    # no time
    parser.add_argument('-no_time', '--no_time', default=False, action='store_true',
                        help='''Dont add time to model folder. To keep the folder name same in different start time''')
    # merge library
    parser.add_argument('-m', '--merge', default=False, action='store_true',
                        help='''Merge all predicted data to one library or not (the individual ones will still be kept)''')
    return parser


def exit_in_preprocess_step(preprocess_msgs):
    for msg in preprocess_msgs:
        print(msg)
    exit(-1)


def parse_args_from_cmd_to_runner(parser, time):
    inputs = copy.deepcopy(parser.parse_args().__dict__)
    arg_msgs = []

    # Check and set work folder
    work_dir = inputs['work_dir']
    if work_dir is None:
        work_dir = join_path(os.path.dirname(os.path.abspath(__file__)), f'{time}-DeepPhospho-WorkFolder')
        arg_msgs.append(f'-w or -work_dir is not defined, use {work_dir} as work directory')
    arg_msgs.append(f'Set work directory to {work_dir}')
    os.makedirs(work_dir, exist_ok=True)

    no_time = inputs['no_time']

    # Check and set task name
    task_name = inputs['task_name']
    if task_name is None:
        task_name = f'Task_{time}'
        arg_msgs.append(f'-t or --task_name is not passed, use {task_name} as task name')
    else:
        if no_time:
            task_name = f'{task_name}'
        else:
            task_name = f'{task_name}_{time}'
    arg_msgs.append(f'Set task name as {task_name}')

    # Get use RT ensembl or not
    ensemble_rt = inputs['ensemble_rt']
    if ensemble_rt:
        arg_msgs.append(f'Use ensemble RT model')
        rt_layers = [4, 5, 6, 7, 8]
    else:
        rt_layers = [8]

    # Task for train and pred
    perform_train = True if inputs['train'] == 1 else False
    perform_pred = True if inputs['predict'] == 1 else False

    # Skip fine-tuning
    skip_ion_finetune = inputs['skip_ion_finetune']
    skip_rt_finetune = {l: inputs[f'skip_rt_finetune_{l}'] for l in rt_layers}

    # Train data split ratio
    train_split_ratio = inputs['train_split_ratio']
    try:
        train_split_ratio = [float(_) for _ in train_split_ratio.split(',')]
        if len(train_split_ratio) not in (2, 3):
            arg_msgs.append(f'ERROR: Training data can only be split into 2 or 3 parts, now {len(train_split_ratio)} parts passed')
            exit_in_preprocess_step(arg_msgs)
        _sum = sum(train_split_ratio)
        train_split_ratio = [_ / _sum for _ in train_split_ratio]
    except ValueError:
        arg_msgs.append(f'ERROR: Cannot convert training data split ratio to digit values, now {train_split_ratio}')
        exit_in_preprocess_step(arg_msgs)
    arg_msgs.append(f'Training data will be split with ratio {":".join([format(_, ".3f") for _ in train_split_ratio])}')

    # Check pre-trained model params
    ion_pretrain = inputs['pretrain_ion_model']
    if ion_pretrain is None:
        ion_pretrain = ''
    ion_pretrain = '' if ion_pretrain == '/' else ion_pretrain
    if ion_pretrain != '' and not os.path.exists(ion_pretrain):
        arg_msgs.append(f'ERROR: Pre-trained ion model is defined but file not found: {ion_pretrain}')
        exit_in_preprocess_step(arg_msgs)
    if skip_ion_finetune and ion_pretrain == '':
        arg_msgs.append(f'ERROR: Skip ion fine-tuning but pre-trained ion parameter file is not defined. '
                        f'Use no -skip_ion_finetune option or define a pre-trained ion parameter file with -pretrain_ion')
        exit_in_preprocess_step(arg_msgs)

    rt_pretrain = {l: inputs[f'pretrain_rt_model_{l}'] for l in rt_layers}
    for l in rt_layers:
        p = rt_pretrain[l]
        if p is None:
            rt_pretrain[l] = ''
        rt_pretrain[l] = '' if rt_pretrain[l] == '/' else rt_pretrain[l]
        if p != '' and not os.path.exists(p):
            arg_msgs.append(f'ERROR: Pre-trained RT model {l} is defined but file not found: {p}')
            exit_in_preprocess_step(arg_msgs)
        if skip_rt_finetune[l] and p == '':
            arg_msgs.append(f'ERROR: Skip RT fine-tuning for layer {l} but pre-trained RT parameter file is not defined. '
                            f'Use no -skip_rt_finetune_{l} option or define a pre-trained RT parameter file with -pretrain_rt_{l}')
            exit_in_preprocess_step(arg_msgs)

    # Get train file, train file type
    train_file = inputs['train_file']
    train_file_type = inputs['train_file_type']

    # Check defined training data
    if perform_train and (train_file is None or train_file == ''):
        arg_msgs.append('ERROR: Perform training but no training data file is defined. '
                        'Use -tt and -tf to define training data and format or use "-train 0" to skip training step')
        exit_in_preprocess_step(arg_msgs)
    if train_file is not None and train_file != '':
        if not os.path.exists(train_file):
            arg_msgs.append(f'ERROR: Training data file is defined but file not found {train_file}')
            exit_in_preprocess_step(arg_msgs)
        if train_file_type is None or train_file_type.lower() not in ['snlib', 'mq1.5', 'mq1.6', 'easypqp']:
            raise ValueError(f'ERROR: File type of training data should be one of ["SNLib", "MQ1.5", "MQ1.6", "EasyPQP"], now {train_file_type}')
        arg_msgs.append(f'Training data with {train_file_type} format: {train_file}')

    # Check defined prediction data (raise error if any prediction file is not correctly defined)
    pred_files = rk.sum_list(inputs['pred_file'])
    pred_files_type = rk.sum_list(inputs['pred_file_type'])
    if perform_pred:
        pred_file_num = len(pred_files)
        for file_idx, file in enumerate(pred_files, 1):
            if not os.path.exists(file):
                raise FileNotFoundError(f'ERROR: Prediction file {file_idx}/{pred_file_num} not found - {file}')

        pred_file_type_num = len(pred_files_type)
        if pred_file_type_num != 1 and pred_file_type_num != pred_file_num:
            arg_msgs.append(f'ERROR: Get {pred_file_num} prediction files but {pred_file_type_num} file types\n'
                            f'There should be either 1 prediction file type, which will be assigned to all prediction files, or same number as prediction files')
            exit_in_preprocess_step(arg_msgs)
        elif pred_file_num != 1 and pred_file_type_num == 1:
            pred_files_type = pred_files_type * pred_file_num
            arg_msgs.append(f'Get {pred_file_num} prediction files and 1 file type. '
                            f'{pred_files_type[0]} will be assigned to all files\n')
        else:
            arg_msgs.append(f'Get {pred_file_num} prediction files and {pred_file_type_num} file types\n')
        files_str = '\n'.join(f'\t{t}: {file}' for t, file in zip(pred_files_type, pred_files))
        arg_msgs.append(f'{files_str}')

    device = inputs['device']
    arg_msgs.append(f'Set device to {device}')

    epoch = inputs['epoch']
    ion_epoch = inputs['ion_epoch']
    rt_epoch = inputs['rt_epoch']
    if ion_epoch is None:
        ion_epoch = epoch
        arg_msgs.append(f'Set epoch of ion model to {ion_epoch}')
    if rt_epoch is None:
        rt_epoch = epoch
        arg_msgs.append(f'Set epoch of RT model to {rt_epoch}')

    ion_bs = inputs['ion_batch_size']
    arg_msgs.append(f'Set batch size of ion model to {ion_bs}')
    rt_bs = inputs['rt_batch_size']
    arg_msgs.append(f'Set batch size of RT model to {rt_bs}')

    init_lr = inputs['learning_rate']
    arg_msgs.append(f'Set initial learning rate to {init_lr}')

    max_pep_len = inputs['max_len']
    arg_msgs.append(f'Set max peptide length to {max_pep_len}')

    rt_scale = list(map(int, inputs['rt_scale'].replace('*', '').split(',')))
    arg_msgs.append(f'Set RT scale from {rt_scale[0]} to {rt_scale[1]}')

    merge = inputs['merge']
    if merge:
        arg_msgs.append(f'Merge all predicted spectral libraries to one after prediction done')

    return arg_msgs, {
        'WorkDir': work_dir,
        'TaskName': task_name,
        'Data-Train': train_file,
        'DataFormat-Train': train_file_type,
        'Data-Pred': pred_files,
        'DataFormat-Pred': pred_files_type,

        'Task-Train': perform_train,
        'Task-Predict': perform_pred,
        'PretrainModel-Ion': ion_pretrain,
        'PretrainModel-RT': rt_pretrain,
        'SkipFinetune-Ion': skip_ion_finetune,
        'SkipFinetune-RT': skip_rt_finetune,

        'Device': device,
        'Epoch-Ion': ion_epoch,
        'Epoch-RT': rt_epoch,
        'BatchSize-Ion': ion_bs,
        'BatchSize-RT': rt_bs,
        'InitLR': init_lr,
        'TrainSplitRatio': train_split_ratio,
        'MaxPepLen': max_pep_len,
        'RTScale': rt_scale,
        'EnsembleRT': ensemble_rt,
        'NoTime': no_time,
        'Merge': merge,

    }


if __name__ == '__main__':
    start_time = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    arg_parser = init_arg_parser()
    msgs, args = parse_args_from_cmd_to_runner(arg_parser, start_time)

    DeepPhosphoRunner(args, start_time=start_time, msgs_for_arg_parsing=msgs)
