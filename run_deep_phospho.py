import argparse
import copy
import datetime
import os
from os.path import join as join_path

from deep_phospho.proteomics_utils import rapid_kit as rk
from deep_phospho.train_pred_utils.runner import DeepPhosphoRunner

HelpMSG = '''
This script integrates all functions of DeepPhospho in one.
To run this, at least two files are needed:
    1. a file contains both spectra and (i)RT, and the following files are valid:
        I. spectral library from Spectronaut (either directDIA or DDA is fine)
        II. msms.txt file generated by MaxQuant
        [for more information about training data, please see -tf (--train_file) and -tt (--train_file_type)]
    2. a file contains under-predicted peptide precursors, and the following files are valid:
        I. spectral library from Spectronaut (either directDIA or DDA)
        II. search result from Spectronaut
        III. msms.txt or evidence.txt file generated by MaxQuant
        IV. two-column tab-separated file with column name "sequence" and "charge" (Some common peptide formats are supported)
        [for more information about prediction input data, please see -pf (--pred_file) and -pt (--pred_file_type)]
We also provided some suggestions about the preperation of these two files, please visit our GitHub repository for more information.

The detailed help message about each argument is listed below.
If you have any question or any suggestion, please contact us.

At last, thank you for using DeepPhospho
[DeepPhospho repository] https://github.com/weizhenFrank/DeepPhospho

'''


def init_arg_parser():
    parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter, description=HelpMSG)

    # work folder
    parser.add_argument('-w', '--work_dir', metavar='directory', type=str, default=None,
                        help='Task will start in this directory')
    # task name
    parser.add_argument('-t', '--task_name', metavar='str', type=str, default=None,
                        help='Task name will be used to name some folders')

    # train file
    parser.add_argument('-tf', '--train_file', metavar='path', type=str, default=None,
                        help='''Train file can be either a spectral library from Spectronaut or msms.txt from MaxQuant''')
    # train file type
    parser.add_argument('-tt', '--train_file_type', metavar='str', default=None,
                        help='''To use Spectronaut library, set this to "SNLib"
Use MaxQuant msms.txt file, set this to "MQ1.5" for MaxQuant version <= 1.5, and "MQ1.6" for version >= 1.6''')
    # pred file
    parser.add_argument('-pf', '--pred_file', metavar='path', type=str, required=True, nargs='*', action='append',
                        help='''File contains under-predicted peptide precursors may has multi-source:
I. spectral library from Spectronaut
II. search result from Spectronaut
III. msms.txt or evidence.txt from MaxQuant
IV. any tab-separated file with two columns "sequence" and "charge"
Either -pf file_1 file_2 ... or -pf file_1 -pf file_2 ... or mix above two is fine for this argument''')
    # pred file type
    parser.add_argument('-pt', '--pred_file_type', metavar='str', type=str, required=True, nargs='*', action='append',
                        help='''The source of prediction input file or peptide format
I. "SNLib" for Spectronaut library
II. "SNResult" for Spectronaut result
III. "MQ1.5" or "MQ1.6" for msms.txt/evidence.txt from MaxQuant version <= 1.5 or >= 1.6
IV. for peptide list file, the modified peptides in the following format are valid
    a. "PepSN13" is Spectronaut 13+ peptide format like _[Acetyl (Protein N-term)]M[Oxidation (M)]LSLS[Phospho (STY)]PLK_
    b. "PepMQ1.5" is MaxQuant 1.5- peptide format like _(ac)GS(ph)QDM(ox)GS(ph)PLRET(ph)RK_
    c. "PepMQ1.6" is MaxQuant 1.6+ peptide format like _(Acetyl (Protein N-term))TM(Oxidation (M))DKS(Phospho (STY))ELVQK_
    d. "PepComet" is Comet peptide format like n#DFM*SPKFS@LT@DVEY@PAWCQDDEVPITM*QEIR
    e. "PepDP" is DeepPhospho used peptide format like *1ED2MCLK
When -pt is passed once and multi -pf are used, the single format will be assigned to all input files. 
If the input files have different formats, the same number of -pt is needed''')

    # device
    parser.add_argument('-d', '--device', metavar='cpu|0|1|...', type=str, default='cpu',
                        help='Use which device. This can be [cpu] or any integer (0, 1, 2, ...) to use corresponded GPU')
    # epoch
    parser.add_argument('-e', '--epoch', metavar='int', type=int, default=30,
                        help='Train how much epochs for both ion an RT models. '
                             'This will only be effective for one of ion or RT model when -ie (--ion_epoch) or -re (--rt_epoch) is provided. '
                             'Or no effect when both -ie and -re are provided. '
                             'Default is 30')
    parser.add_argument('-ie', '--ion_epoch', metavar='int', type=int, default=None,
                        help='Train how much epochs on ion model. Default is 30')
    parser.add_argument('-re', '--rt_epoch', metavar='int', type=int, default=None,
                        help='Train how much epochs on RT model. Default is 30')
    # batch size
    parser.add_argument('-ibs', '--ion_batch_size', metavar='int', type=int, default=64,
                        help='Batch size for ion model training. Default is 64')
    parser.add_argument('-rbs', '--rt_batch_size', metavar='int', type=int, default=128,
                        help='Batch size for RT model training. Default is 128')
    # initial learning rate
    parser.add_argument('-lr', '--learning_rate', metavar='float', type=float, default=0.0001,
                        help='Initial learning rate for two models. '
                             'Default is 0.0001 while a smaller value is recommanded if the size of training data is small (e.g. hundreds of precursors)')
    # pep length
    parser.add_argument('-ml', '--max_len', metavar='int', type=int, default=74,
                        help='The max peptide length for input dataset. '
                             'Default is 74 to avoid error but 54 (or 25) is recommended as default setting of Spectronaut (or Skyline)')
    # rt scale
    parser.add_argument('-rs', '--rt_scale', metavar='low,high', type=str, default='*-100,200',
                        help='Define the lower and upper limitations for RT model. '
                             'Separate two numbers with a comma like `0,10`. '
                             'And a `*` will be needed to pass a negative number, like *-100,200. '
                             'Default is -100 to 200.')
    # rt ensemble
    parser.add_argument('-en', '--rt_ensemble', default=False, action='store_true',
                        help='Use ensemble to improve RT prediction or not')
    # pre-trained models
    # TODO pre-train

    # pre-defined models
    # TODO differetiate no fine-tuning with pre-train
    parser.add_argument('-ion_model', '--exist_ion_model', metavar='path', type=str, default=None,
                        help='Use existing ion model parameters instead of training a new one. '
                             'When this argument is passed, ion model fine-tuning step will be skipped')
    for l in [4, 5, 6, 7, 8]:
        parser.add_argument(f'-rt_model_{l}', f'--exist_rt_model_{l}', metavar='path', type=str, default=None,
                            help=f'Use existing RT model parameters (with {l} encoder layer) instead of training a new one. '
                                 f'When this argument is passed, RT model fine-tuning step for {l} encoder layer will be skipped. '
                                 f'If -en (-rt_ensemble) is not used, only -rt_model_8 is required in the case to skip RT model fine-tuning')
    # no time
    parser.add_argument('-no_time', '--no_time', default=False, action='store_true',
                        help='''Dont add time to model folder. To keep the folder name same in different start time''')
    # merge library
    parser.add_argument('-m', '--merge', default=False, action='store_true',
                        help='''Merge all predicted data to one library or not (the individual ones will still be kept)''')
    return parser


def exit_in_preprocess_step(preprocess_msgs):
    for msg in preprocess_msgs:
        print(msg)
    exit(-1)


def parse_args(parser, time):
    inputs = copy.deepcopy(parser.parse_args().__dict__)
    arg_msgs = []

    # Check and set work folder
    work_dir = inputs['work_dir']
    if work_dir is None:
        work_dir = join_path(os.path.dirname(os.path.abspath(__file__)), f'{time}-DeepPhospho-WorkFolder')
        arg_msgs.append(f'-w or -work_dir is not passed, use {work_dir} as work directory')
    else:
        arg_msgs.append(f'Set work directory to {work_dir}')
    os.makedirs(work_dir, exist_ok=True)

    no_time = inputs['no_time']

    # Check and set task name
    task_name = inputs['task_name']
    if task_name is None:
        task_name = f'Task_{time}'
        arg_msgs.append(f'-t or --task_name is not passed, use {task_name} as task name')
    else:
        if no_time:
            task_name = f'{task_name}'
        else:
            task_name = f'{task_name}_{time}'
        arg_msgs.append(f'Set task name to {task_name}')

    # Get use RT ensembl or not
    rt_ensemble = inputs['rt_ensemble']
    if rt_ensemble:
        arg_msgs.append(f'Use ensemble RT model')

    # Get train file, train file type and existed ion and RT models
    train_file = inputs['train_file']
    train_file_type = inputs['train_file_type']
    existed_ion_model = inputs['exist_ion_model']
    existed_rt_model = {l: inputs[f'exist_rt_model_{l}'] for l in [4, 5, 6, 7, 8]}
    skip_ion_finetune = False
    skip_rt_finetune = False

    # Check passed ion and RT models are existed if train file is not provided
    if train_file is None:
        if existed_ion_model is None:
            arg_msgs.append('ERROR: No training data is passed and no existed ion model. '
                            'Use -tt and -tf to define training data and format or use -ion_model to define an existed ion model')
            exit_in_preprocess_step(arg_msgs)
        elif any([_ is None for _ in existed_rt_model.values()]) and rt_ensemble:
            arg_msgs.append('ERROR: No training data is passed and not all 5 rt models existed for rt ensemble. '
                            'May not use -en if only one rt model is expected')
            exit_in_preprocess_step(arg_msgs)
        elif existed_rt_model[8] is None and (not rt_ensemble):
            arg_msgs.append('ERROR: No training data is passed and no existed rt model. '
                            'Use -tt and -tf to define training data and format or use -rt_model_[45678] to define an existed ion model')
            exit_in_preprocess_step(arg_msgs)
        else:
            arg_msgs.append(f'Use existed ion and rt models')
            arg_msgs.append(f'\tUse existed ion model: {existed_ion_model}')
            if not rt_ensemble:
                arg_msgs.append(f'\tUse existed RT model: {existed_rt_model[8]}')
            else:
                for l, p in existed_rt_model.items():
                    arg_msgs.append(f'\tUse existed RT model {l}: {p}')

        skip_ion_finetune = True
        skip_rt_finetune = True

    # When train file is provided, the check of existed ion and RT models will be delayed to next check point
    else:
        if not os.path.exists(train_file):
            raise FileNotFoundError(f'ERROR: Train file not found - {train_file}')
        if train_file_type is None or train_file_type.lower() not in ['snlib', 'mq1.5', 'mq1.6']:
            raise ValueError(f'ERROR: Train file type should be one of ["SNLib", "MQ1.5", "MQ1.6"], now {train_file_type}')
        arg_msgs.append(f'Train file with {train_file_type} format: {train_file}')

    # Check if existed ion model file is existed
    if existed_ion_model is not None:
        if os.path.exists(existed_ion_model):
            arg_msgs.append(f'Use existed ion model: {existed_ion_model}')
            skip_ion_finetune = True
        else:
            arg_msgs.append(f'ERROR: Existed ion model is passed but file not found: {existed_ion_model}')
            exit_in_preprocess_step(arg_msgs)

    # Check if existed rt model file is existed for case of no rt ensemble
    if existed_rt_model[8] is not None and (not rt_ensemble):
        if os.path.exists(existed_rt_model[8]):
            arg_msgs.append(f'Use existed rt model with no rt ensemble: {existed_rt_model[8]}')
            skip_rt_finetune = True
        else:
            arg_msgs.append(f'ERROR: Existed rt model is passed but file not found: {existed_rt_model[8]}')
            exit_in_preprocess_step(arg_msgs)

    # Check if any existed rt model file is existed for case of rt ensemble
    if any([_ is not None for _ in existed_rt_model.values()]) and rt_ensemble:
        skip_rt_finetune = []
        for l, p in existed_rt_model.items():
            if p is None:
                continue
            if os.path.exists(p):
                arg_msgs.append(f'Use existed rt model ({l}): {p}')
                skip_rt_finetune.append(l)
            else:
                arg_msgs.append(f'ERROR: Existed rt model is passed but file not found: {p}')
                exit_in_preprocess_step(arg_msgs)

    pred_files = rk.sum_list(inputs['pred_file'])
    for file_idx, file in enumerate(pred_files, 1):
        if not os.path.exists(file):
            raise FileNotFoundError(f'ERROR: Prediction file {file_idx} not found - {file}')

    pred_files_type = rk.sum_list(inputs['pred_file_type'])
    pred_file_type_num = len(pred_files_type)
    pred_file_num = len(pred_files)
    if pred_file_type_num != 1 and pred_file_type_num != pred_file_num:
        raise ValueError(f'ERROR: Get {pred_file_num} prediction files but {pred_file_type_num} file type\n')
    elif pred_file_num != 1 and pred_file_type_num == 1:
        pred_files_type = pred_files_type * pred_file_num
        msg = (f'Get {pred_file_num} prediction files and 1 file type. '
               f'{pred_files_type[0]} will be assigned to all files\n')
    else:
        msg = f'Get {pred_file_num} prediction files and {pred_file_type_num} file type\n'
    files_str = '\n'.join(f'\t{t}: {file}' for t, file in zip(pred_files_type, pred_files))
    arg_msgs.append(f'{msg}{files_str}')

    device = inputs['device']
    arg_msgs.append(f'Set device to {device}')

    epoch = inputs['epoch']
    ion_epoch = inputs['ion_epoch']
    rt_epoch = inputs['rt_epoch']
    if ion_epoch is None:
        ion_epoch = epoch
        arg_msgs.append(f'Set epoch of ion model to {ion_epoch}')
    if rt_epoch is None:
        rt_epoch = epoch
        arg_msgs.append(f'Set epoch of RT model to {rt_epoch}')

    ion_bs = inputs['ion_batch_size']
    arg_msgs.append(f'Set batch size of ion model to {ion_bs}')
    rt_bs = inputs['rt_batch_size']
    arg_msgs.append(f'Set batch size of RT model to {rt_bs}')

    init_lr = inputs['learning_rate']
    arg_msgs.append(f'Set initial learning rate to {init_lr}')

    max_pep_len = inputs['max_len']
    arg_msgs.append(f'Set max peptide length to {max_pep_len}')

    rt_scale = list(map(int, inputs['rt_scale'].replace('*', '').split(',')))
    arg_msgs.append(f'Set RT scale from {rt_scale[0]} to {rt_scale[1]}')

    merge = inputs['merge']
    if merge:
        arg_msgs.append(f'Merge all predicted spectral libraries to one after prediction done')

    return arg_msgs, {
        'WorkDir': work_dir,
        'TaskName': task_name,
        'TrainData': (train_file, train_file_type),
        'PredData': list(zip(pred_files, pred_files_type)),
        'ExistedIonModel': existed_ion_model,
        'ExistedRTModel': existed_rt_model,
        'SkipIonFinetune': skip_ion_finetune,
        'SkipRTFinetune': skip_rt_finetune,
        'Device': device,
        'IonEpoch': ion_epoch,
        'RTEpoch': rt_epoch,
        'IonBatchSize': ion_bs,
        'RTBatchSize': rt_bs,
        'InitLR': init_lr,
        'MaxPepLen': max_pep_len,
        'RTScale': rt_scale,
        'EnsembleRT': rt_ensemble,
        'NoTime': no_time,
        'Merge': merge
    }


if __name__ == '__main__':
    start_time = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    arg_parser = init_arg_parser()
    msgs, args = parse_args(arg_parser, start_time)

    DeepPhosphoRunner(args, start_time=start_time, msgs_for_arg_parsing=msgs)
