{
    "WorkFolder": "Here",
    "ExpName": "U2OS_DIA_Predict",
    "TaskPurpose": "Predict",
    "PretrainParam": "./demo/NameOfTaskInfo/ckpts/best_model.pth",
    "Intensity_DATA_PREPROCESS_CFG": {
        "MAX_SEQ_LEN": 52
    },
    "Intensity_DATA_CFG": {
        "DataName": "U2OS_DIA",
        "TrainPATH": "",
        "TestPATH": "",
        "HoldoutPATH": "",
        "PredInputPATH": "./demo/U2OS_DIA_demo_data/20201010-Inten_Holdout-U2OS_DIA-seed0_811.json",
        "InputWithLabel": true,
        "Intensity_FIELD_NAME": "normalized_intensity",
        "SEQUENCE_FIELD_NAME": "sequence",
        "PRECURSOR_CHARGE": "charge",
        "DATA_PROCESS_CFG": {
            "MAX_SEQ_LEN": 52
        },
        "refresh_cache": true
    },
    "MODEL_CFG": {
        "model_name": "LSTMTransformer",
        "embed_dim": 256,
        "lstm_hidden_dim": 512,
        "lstm_layers": 2,
        "lstm_num": 2,
        "bidirectional": true,
        "max_len": 100,
        "num_attention_head": 8,
        "fix_lstm": false,
        "pos_encode_dropout": 0.1,
        "attention_dropout": 0.1,
        "num_encd_layer": 8,
        "transformer_hidden_dim": 1024
    },
    "Ensemble_MODEL_CFG": {
        "model_name": "LSTMTransformerEnsemble",
        "embed_dim": 256,
        "lstm_hidden_dim": 512,
        "lstm_layers": 2,
        "lstm_num": 2,
        "bidirectional": true,
        "max_len": 100,
        "num_attention_head": 8,
        "fix_lstm": false,
        "pos_encode_dropout": 0.1,
        "attention_dropout": 0.1,
        "transformer_hidden_dim": 1024
    },
    "UsedModelCFG": {
        "model_name": "LSTMTransformer",
        "embed_dim": 256,
        "lstm_hidden_dim": 512,
        "lstm_layers": 2,
        "lstm_num": 2,
        "bidirectional": true,
        "max_len": 100,
        "num_attention_head": 8,
        "fix_lstm": false,
        "pos_encode_dropout": 0.1,
        "attention_dropout": 0.1,
        "num_encd_layer": 8,
        "transformer_hidden_dim": 1024
    },
    "TRAINING_HYPER_PARAM": {
        "resume": false,
        "Bert_pretrain": false,
        "accumulate_mask_only": false,
        "DEBUG": false,
        "lr_scheduler_type": "WarmupMultiStepLR",
        "LR": 0.0001,
        "transformer_on_epoch": -1,
        "factor": 0.2,
        "warmup_factor": 0.3333333333333333,
        "warmup_steps": 5000,
        "weight_decay": 1e-08,
        "warmup_iters": 0,
        "save_param_interval": 300,
        "GPU_INDEX": "0",
        "module_namelist": null,
        "remove_ac_pep": false,
        "loss_func": "MSE",
        "LR_STEPS": [
            2000,
            6000
        ],
        "BATCH_SIZE": 128,
        "EPOCH": 30,
        "use_prosit_pretrain": false,
        "two_stage": true,
        "lambda_cls": 0.0,
        "pdeep2mode": true,
        "inter_layer_prediction": false,
        "add_phos_principle": true,
        "use_all_data": false,
        "only_two_ions": false
    },
    "TEST_HYPER_PARAM": {
        "Use multiple iteration": false
    }
}